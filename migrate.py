#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PostgreSQL â†’ MongoDB æ´»å‹•è³‡æ–™åˆå§‹åŒ–é·ç§»å·¥å…·ï¼Œå…·å‚™ï¼š
- ä»¥ä¸»è¡¨ç‚ºè¨ˆæ•¸å–®ä½ï¼Œé—œè¯è¡¨ç•°å¸¸ç½®ç©ºã€ä¸çµ‚æ­¢é·ç§»
- é›™éµæ–·é»ï¼ˆæ™‚é–“+event_noï¼‰çºŒå‚³
- ç²—ç²’åº¦å¤§çª—å£æ‰¹æ¬¡+å½™ç¸½ä¸€è‡´æ€§é©—è­‰
- æ—¥èªŒå»¶é²åˆå§‹åŒ–é¿å…ç„¡åƒæ•¸åŸ·è¡Œç”¢ç”Ÿç©ºæª”
- å¤šç·šç¨‹æ‰¹é‡é·ç§»ï¼Œæé«˜æ•ˆèƒ½
- å‘½ä»¤è¡Œå¤šæ¨¡å¼æ”¯æ´ï¼ˆå…¨é‡ã€å¢é‡ã€è£œå¯«ã€æ–·é»æ¢å¾©ç­‰ï¼‰
"""

import os
import sys
import json
import logging
import datetime
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed

import psycopg2
from psycopg2.extras import DictCursor
from pymongo import MongoClient, UpdateOne
from dotenv import load_dotenv

# ========== è¼‰å…¥ .env é…ç½® ==========
load_dotenv()

# è®€å–ç›®å‰ç’°å¢ƒï¼Œé è¨­ç‚º dev
ENV = os.getenv("ENV", "dev")

POSTGRESQL_CONFIGS = {
    "dev": {
        "host": os.getenv("PG_HOST_DEV"),
        "port": int(os.getenv("PG_PORT_DEV", 5432)),
        "user": os.getenv("PG_USER_DEV"),
        "password": os.getenv("PG_PASSWORD_DEV"),
        "dbname": os.getenv("PG_DBNAME_DEV"),
    },
    "prod": {
        "host": os.getenv("PG_HOST_PROD"),
        "port": int(os.getenv("PG_PORT_PROD", 5432)),
        "user": os.getenv("PG_USER_PROD"),
        "password": os.getenv("PG_PASSWORD_PROD"),
        "dbname": os.getenv("PG_DBNAME_PROD"),
    }
}

MONGO_URIS = {
    "dev": os.getenv("MONGO_URI_DEV"),
    "prod": os.getenv("MONGO_URI_PROD"),
}

BATCH_SIZE = int(os.getenv("BATCH_SIZE", "500"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "8"))
SCHEMA = "gift2022"

POSTGRESQL_CONFIG = POSTGRESQL_CONFIGS[ENV]
MONGO_URI = MONGO_URIS[ENV]

CHECKPOINT_FILE = "migration_checkpoint.json"
LOG_DIR = "logs"

LOG_FILE = None
logger = None

# ========= å»¶é²åˆå§‹åŒ–æ—¥èªŒ ==========
def init_logger():
    global LOG_FILE, logger
    os.makedirs(LOG_DIR, exist_ok=True)
    LOG_FILE = os.path.join(LOG_DIR, f"migration_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    logging.basicConfig(
        filename=LOG_FILE,
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)s: %(message)s"
    )
    logger = logging.getLogger()

# ========= å…¨å±€è³‡æ–™åº«é€£ç·š ==========
pg_conn = None
pg_cursor = None
mongo_client = None
col_events = None
col_attendees = None

def init_db_conn():
    """ åˆå§‹åŒ– PostgreSQL èˆ‡ MongoDB é€£ç·š """
    global pg_conn, pg_cursor, mongo_client, col_events, col_attendees
    try:
        pg_conn = psycopg2.connect(**POSTGRESQL_CONFIG, cursor_factory=DictCursor)
        pg_cursor = pg_conn.cursor()
    except Exception as e:
        logger.error(f"ç„¡æ³•é€£æ¥ PostgreSQLï¼š{e}")
        sys.exit(1)

    try:
        mongo_client = MongoClient(MONGO_URI)
        mdb = mongo_client["sogo_gift"]
        col_events = mdb["events"]
        col_attendees = mdb["event_attendees"]

        col_events.create_index([
            ("hccEventType", 1), ("eventStatus", 1), ("onlyApp", 1),
            ("startDate", 1), ("endDate", 1), ("eventBranchId", 1)
        ])
        col_events.create_index("memberTypes")
        col_attendees.create_index([("eventNo", 1), ("appId", 1)], unique=True)
    except Exception as e:
        logger.error(f"ç„¡æ³•é€£æ¥ MongoDBï¼š{e}")
        sys.exit(1)

BRANCH_ORDER = {
    "BRANCH_TAIWAN": "å…¨å°", "BRANCH_TAIPEI": "å°åŒ—åº—", "BRANCH_JS": "å¿ å­é¤¨",
    "BRANCH_FS": "å¾©èˆˆé¤¨", "BRANCH_DH": "æ•¦åŒ–é¤¨", "BRANCH_TM": "å¤©æ¯åº—",
    "BRANCH_JL": "ä¸­å£¢åº—", "BRANCH_BC": "æ–°ç«¹åº—", "BRANCH_GS": "é«˜é›„åº—",
    "BRANCH_GC": "Garden City", "BRANCH_GCA": "Garden City-Aå€",
    "BRANCH_GCB": "Garden City-Bå€", "BRANCH_GCC": "Garden City-Cå€", "BRANCH_GCD": "Garden City-Då€"
}

# ========= ä¸»è¡¨èˆ‡å­è¡¨æ•¸æ“šæ“´å…… ==========
def enrich_event(event):
    event_no = event["eventNo"]
    try:
        sql = f"SELECT member_type FROM {SCHEMA}.gif_hcc_event_member_type WHERE event_no = %s"
        logger.info(f"[SQL] æŸ¥æœƒå“¡é¡å‹: {sql} åƒæ•¸: [{event_no}]")
        pg_cursor.execute(sql, (event_no,))
        rows = pg_cursor.fetchall()
        event["memberTypes"] = [r["member_type"] for r in rows] if rows else []
    except Exception as e:
        logger.warning(f"æœƒå“¡é¡å‹æŸ¥è©¢å¤±æ•—ï¼Œäº‹ä»¶ {event_no}ï¼ŒéŒ¯èª¤ï¼š{e}")
        event["memberTypes"] = []

    try:
        sql = f"""
            SELECT DISTINCT branch
            FROM {SCHEMA}.gif_event_coupon_burui
            WHERE coupon_setting_no IN (
                SELECT coupon_setting_no FROM {SCHEMA}.gif_event_coupon_setting WHERE event_no = %s
            )
        """
        logger.info(f"[SQL] æŸ¥å„ªæƒ åˆ¸åˆ†é¤¨: åƒæ•¸ [{event_no}]")
        pg_cursor.execute(sql, (event_no,))
        branches = [r["branch"] for r in pg_cursor.fetchall()]
        if branches:
            sorted_ids = sorted(branches, key=lambda b: list(BRANCH_ORDER.keys()).index(b) if b in BRANCH_ORDER else 9999)
            event["usingBranchIds"] = sorted_ids
            event["usingBranchNames"] = [BRANCH_ORDER.get(b, b) for b in sorted_ids]
        else:
            event["usingBranchIds"] = []
            event["usingBranchNames"] = []
    except Exception as e:
        logger.warning(f"å„ªæƒ åˆ¸åˆ†é¤¨æŸ¥è©¢å¤±æ•—ï¼Œäº‹ä»¶ {event_no}ï¼ŒéŒ¯èª¤ï¼š{e}")
        event["usingBranchIds"] = []
        event["usingBranchNames"] = []

    event["onlyApp"] = event.get("onlyApp", "N") == 'Y'
    event["allMember"] = event.get("allMember", "N") == 'Y'

    return event

# ========= æ‰¹æ¬¡æŠ“å–æ´»å‹•event_no ==========
def fetch_events_batch(start_time, end_time, last_checkpoint_time=None, last_checkpoint_id=None, batch_size=100):
    if isinstance(start_time, str):
        start_time = datetime.datetime.fromisoformat(start_time)
    if isinstance(end_time, str):
        end_time = datetime.datetime.fromisoformat(end_time)

    if last_checkpoint_time is not None and last_checkpoint_id is not None:
        sql = f"""
            SELECT event_no
            FROM {SCHEMA}.gif_event
            WHERE (exchange_start_date > %s OR (exchange_start_date = %s AND event_no > %s))
              AND exchange_start_date >= %s AND exchange_start_date < %s
            ORDER BY exchange_start_date, event_no
            LIMIT %s
        """
        params = [last_checkpoint_time, last_checkpoint_time, last_checkpoint_id, start_time, end_time, batch_size]
    else:
        sql = f"""
            SELECT event_no
            FROM {SCHEMA}.gif_event
            WHERE exchange_start_date >= %s AND exchange_start_date < %s
            ORDER BY exchange_start_date, event_no
            LIMIT %s
        """
        params = [start_time, end_time, batch_size]

    logger.info(f"[SQL] fetch_events_batch:\n{sql}\nåƒæ•¸: {params}")
    pg_cursor.execute(sql, params)
    rows = pg_cursor.fetchall()
    return [row["event_no"] for row in rows]

def fetch_event(event_no):
    sql = f"""
        SELECT he.event_no, he.event_memo, he.branch AS eventBranchId, he.prize_coupon_json,
               he.gift_attention_url, he.only_app, he.all_member, he.hcc_event_type,
               e.name, e.event_status, e.exchange_start_date, e.exchange_end_date, e.web_url
        FROM {SCHEMA}.gif_hcc_event he
        JOIN {SCHEMA}.gif_event e ON he.event_no = e.event_no
        WHERE he.event_no = %s
    """
    logger.info(f"[SQL] fetch_event {event_no}")
    pg_cursor.execute(sql, (event_no,))
    row = pg_cursor.fetchone()
    if not row:
        return None
    keys = ["eventNo", "eventMemo", "eventBranchId", "prizeCouponJson", "giftAttentionUrl", "onlyApp", "allMember", "hccEventType",
            "name", "eventStatus", "startDate", "endDate", "giftInforUrl"]
    return dict(zip(keys, row))

def migrate_attendees(event_no):
    try:
        sql = f"SELECT app_id FROM {SCHEMA}.gif_hcc_event_attendee WHERE event_no = %s"
        logger.info(f"[SQL] migrate_attendees {event_no}")
        pg_cursor.execute(sql, (event_no,))
        rows = pg_cursor.fetchall()
        if not rows:
            return 0
        requests = []
        for r in rows:
            app_id = r.get("app_id")
            if app_id:
                requests.append(UpdateOne(
                    {"eventNo": event_no, "appId": app_id},
                    {"$setOnInsert": {"eventNo": event_no, "appId": app_id}},
                    upsert=True
                ))
        if not requests:
            return 0
        result = col_attendees.bulk_write(requests, ordered=False)
        return result.upserted_count
    except Exception as e:
        logger.warning(f"åƒèˆ‡è€…è³‡æ–™å¯«å…¥å¤±æ•—ï¼Œäº‹ä»¶ {event_no}ï¼ŒéŒ¯èª¤ï¼š{e}")
        return 0

def migrate_single_event(event_no):
    try:
        event = fetch_event(event_no)
        if not event:
            logger.warning(f"æ‰¾ä¸åˆ°æ´»å‹• {event_no}")
            return False
        event = enrich_event(event)
        col_events.replace_one({"eventNo": event_no}, event, upsert=True)
        inserted = migrate_attendees(event_no)
        logger.info(f"æ´»å‹• {event_no} é·ç§»æˆåŠŸï¼Œåƒèˆ‡è€… {inserted} ç­†")
        return True
    except Exception as e:
        logger.error(f"æ´»å‹• {event_no} é·ç§»å¤±æ•—ï¼ŒéŒ¯èª¤ï¼š{e}")
        return False

def count_pg_events_in_window(start, end):
    if isinstance(start, str):
        start = datetime.datetime.fromisoformat(start)
    if isinstance(end, str):
        end = datetime.datetime.fromisoformat(end)
    sql = f"SELECT COUNT(*) FROM {SCHEMA}.gif_hcc_event he JOIN {SCHEMA}.gif_event e ON he.event_no = e.event_no WHERE e.exchange_start_date >= %s AND e.exchange_start_date < %s"
    pg_cursor.execute(sql, (start, end))
    return pg_cursor.fetchone()[0]

def migrate_window(window: dict, window_name: str):
    logger.info(f"è™•ç†çª—å£ {window_name}ï¼š{window['start']} ~ {window['end']} ç‹€æ…‹ï¼š{window['status']}")
    if window.get("status") == "completed":
        logger.info(f"{window_name} å·²å®Œæˆï¼Œè·³é")
        return 0

    last_time = window.get("last_checkpoint_time")
    last_id = window.get("last_checkpoint_id")

    window["status"] = "in_progress"
    window["owner"] = os.uname().nodename
    window["start_exec_time"] = datetime.datetime.now().isoformat()
    save_checkpoint(cp_data)

    migrated = 0
    consecutive_fail_count = 0

    while True:
        batch_event_nos = fetch_events_batch(window["start"], window["end"], last_time, last_id, BATCH_SIZE)
        if not batch_event_nos:
            pg_total = count_pg_events_in_window(window["start"], window["end"])
            logger.info(f"PGä¸»è¡¨ç¸½æ•¸ï¼š{pg_total}ï¼Œå·²æˆåŠŸé·ç§»ï¼š{migrated}")
            if migrated >= pg_total:
                window["status"] = "completed"
                logger.info(f"{window_name} çª—å£å·²å®Œæˆ")
            else:
                window["status"] = "pending"
                logger.warning(f"{window_name} çª—å£å°šæœ‰ {pg_total - migrated} ç­†æœªé·ç§»")
            break

        logger.info(f"{window_name} æ‰¹æ¬¡å¤§å°ï¼š{len(batch_event_nos)}")

        # ä¸²è¡Œè™•ç†ï¼Œç­‰å¾…æ¯ç­†å®Œæˆå†è™•ç†ä¸‹ä¸€ç­†
        for eno in batch_event_nos:
            success = migrate_single_event(eno)
            if success:
                migrated += 1
                last_id = eno
                # æ›´æ–°æœ€å¾Œäº‹ä»¶æ™‚é–“ä½œæ–·é»
                pg_cursor.execute(f"SELECT exchange_start_date FROM {SCHEMA}.gif_event WHERE event_no = %s", (eno,))
                row = pg_cursor.fetchone()
                if row and row["exchange_start_date"]:
                    last_time = row["exchange_start_date"].isoformat()
                else:
                    # ç™¼ç”Ÿå•é¡Œæ™‚ä¸æ›´æ–° last_timeï¼Œä¿ç•™ä¸Šæ¬¡æ–·é»
                    logger.warning(f"å–event_noï¼š{eno} æ™‚é–“å¤±æ•—ï¼Œæ–·é»æ™‚é–“ä¸æ›´æ–°")
                # æ›´æ–°æ–·é»
                window["last_checkpoint_time"] = last_time
                window["last_checkpoint_id"] = last_id
                window["processed_count"] = migrated
                window["last_update_time"] = datetime.datetime.now().isoformat()
                save_checkpoint(cp_data)
                consecutive_fail_count = 0
                logger.info(f"æ´»å‹• {eno} é·ç§»æˆåŠŸï¼Œæ–·é»æ™‚é–“æ›´æ–°: {last_time}")
            else:
                consecutive_fail_count += 1
                logger.warning(f"æ´»å‹• {eno} é·ç§»å¤±æ•—ï¼Œé€£çºŒéŒ¯èª¤è¨ˆæ•¸ {consecutive_fail_count}")
                if consecutive_fail_count >= 5:
                    logger.error(f"{window_name} é€£çºŒ 5 ç­†å¤±æ•—ï¼Œçµ‚æ­¢é·ç§»é¿å…æ­»å¾ªç’°")
                    return migrated

    window["finish_exec_time"] = datetime.datetime.now().isoformat()
    save_checkpoint(cp_data)

    pg_count = count_pg_events_in_window(window["start"], window["end"])
    fail_count = max(pg_count - migrated, 0)
    logger.info(f"\n-- {window_name} çª—å£é·ç§»ç¸½çµ --")
    logger.info(f"çª—æˆ¶æ™‚é–“ç¯„åœ: {window['start']} ~ {window['end']}")
    logger.info(f"PGä¸»è¡¨ç­†æ•¸: {pg_count}")
    logger.info(f"æˆåŠŸé·ç§»ç­†æ•¸: {migrated}")
    logger.info(f"å¤±æ•—ç­†æ•¸: {fail_count}")
    logger.info(f"çª—å£ç‹€æ…‹: {window['status']}\n")

    return migrated

def load_checkpoint():
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"base_windows": [], "correction_windows": []}

def save_checkpoint(data):
    with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# å‘½ä»¤è¡Œç®¡ç†åŠå°æ‡‰æ“ä½œ

def command_full_sync(end_time=None):
    if end_time is None:
        end_time = datetime.datetime.now().isoformat()

    global cp_data
    cp_data.setdefault("base_windows", [])
    exists = any(
        w["start"] == "1970-01-01T00:00:00" and w["end"] == end_time
        for w in cp_data["base_windows"]
    )
    if not exists:
        cp_data["base_windows"].append({
            "start": "1970-01-01T00:00:00",
            "end": end_time,
            "status": "pending",
            "owner": None,
            "processed_count": 0,
            "last_checkpoint_time": None,
            "last_checkpoint_id": None,
            "last_update_time": None,
            "start_exec_time": None,
            "finish_exec_time": None
        })
        save_checkpoint(cp_data)
        logger.info(f"æ–°å¢åŸºç·šçª—å£: 1970-01-01 è‡³ {end_time}")

    total = 0
    for idx, w in enumerate(cp_data["base_windows"]):
        if w["status"] in ("pending", "in_progress"):
            total += migrate_window(w, f"base_windows[{idx}]")
    print(f"å…¨é‡åŒæ­¥å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")

def command_incremental(end_time=None):
    if end_time is None:
        end_time = datetime.datetime.now().isoformat()
    global cp_data
    cp_data.setdefault("base_windows", [])
    for w in cp_data["base_windows"]:
        if w["end"] < end_time:
            w["end"] = end_time
    total = 0
    for idx, w in enumerate(cp_data["base_windows"]):
        if w["status"] in ("pending", "in_progress"):
            total += migrate_window(w, f"base_windows[{idx}]")
    save_checkpoint(cp_data)
    print(f"å¢é‡åŒæ­¥å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")

def command_correction(start_time, end_time, force=False):
    global cp_data
    cp_data.setdefault("correction_windows", [])
    cp_data["correction_windows"].append({
        "start": start_time,
        "end": end_time,
        "status": "pending",
        "owner": None,
        "processed_count": 0,
        "last_checkpoint_time": None,
        "last_checkpoint_id": None,
        "last_update_time": None,
        "resync": force,
        "start_exec_time": None,
        "finish_exec_time": None
    })
    save_checkpoint(cp_data)
    logger.info(f"æ–°å¢è£œå¯«çª—å£ï¼š{start_time} è‡³ {end_time} å¼·åˆ¶è¦†è“‹={force}")

    total = 0
    for idx, w in enumerate(cp_data["correction_windows"]):
        if w["status"] in ("pending", "in_progress"):
            total += migrate_window(w, f"correction_windows[{idx}]")
    print(f"è£œå¯«åŒæ­¥å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")

def command_resume():
    global cp_data
    total = 0
    for wlist in ("base_windows", "correction_windows"):
        for idx, w in enumerate(cp_data.get(wlist, [])):
            if w["status"] in ("pending", "in_progress"):
                total += migrate_window(w, f"{wlist}[{idx}]")
    print(f"æ–·é»æ¢å¾©å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")

def command_show_status():
    global cp_data
    print(json.dumps(cp_data, ensure_ascii=False, indent=2))

def command_reset():
    confirm = input("è­¦å‘Šï¼šé‡ç½®æ–·é»å°‡æ¸…é™¤æ‰€æœ‰é·ç§»é€²åº¦ä¸”ç„¡æ³•æ¢å¾©ï¼Œè«‹è¼¸å…¥ Y ç¢ºèªï¼š")
    if confirm.strip().upper() == "Y":
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
        global cp_data
        cp_data = load_checkpoint()
        logger.info("æ–·é»æª”æ¡ˆå·²é‡ç½®")
        print("é‡ç½®å®Œæˆ")
    else:
        print("æ“ä½œå·²å–æ¶ˆ")

def command_gen_report():
    global cp_data
    print("====== é·ç§»ç‹€æ…‹å ±å‘Š ======")
    for window_type in ("base_windows", "correction_windows"):
        print(f"\n[{window_type}] æ™‚é–“çª—å£:")
        for w in cp_data.get(window_type, []):
            print(f"  æ™‚æ®µ: {w['start']} ~ {w['end']}")
            print(f"  ç‹€æ…‹: {w['status']}, å·²é·ç§»ç­†æ•¸: {w.get('processed_count', 0)}")
            print(f"  æœ€å¾Œæ–·é»æ™‚é–“: {w.get('last_checkpoint_time')}")
            print(f"  æœ€å¾Œæ–·é»äº‹ä»¶ID: {w.get('last_checkpoint_id')}")
            print(f"  åŸ·è¡Œè€…: {w.get('owner')}")
            print(f"  æœ€å¾Œæ›´æ–°æ™‚é–“: {w.get('last_update_time')}")
    print("============================")

def parse_args():
    parser = argparse.ArgumentParser(description="PostgreSQL â†’ MongoDB æ´»å‹•è³‡æ–™é·ç§»å·¥å…·")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--full-sync", action="store_true", help="å…¨é‡åŒæ­¥")
    group.add_argument("--incremental", action="store_true", help="å¢é‡åŒæ­¥")
    group.add_argument("--correction", action="store_true", help="è£œå¯«çª—å£")
    group.add_argument("--resume", action="store_true", help="æ–·é»æ¢å¾©")
    group.add_argument("--show-status", action="store_true", help="æŸ¥çœ‹æ–·é»ç‹€æ…‹")
    group.add_argument("--gen-report", action="store_true", help="ç”Ÿæˆé·ç§»å ±å‘Š")
    group.add_argument("--reset", action="store_true", help="é‡ç½®æ‰€æœ‰æ–·é»")

    parser.add_argument("--start", type=str, default=None, help="è£œå¯«çª—å£èµ·å§‹æ™‚é–“ï¼ˆISO8601æ ¼å¼ï¼‰")
    parser.add_argument("--end", type=str, default=None, help="çµæŸæ™‚é–“ï¼ˆISO8601æ ¼å¼æˆ– nowï¼Œé è¨­ç•¶å‰æ™‚é–“ï¼‰")
    parser.add_argument("--force", action="store_true", help="è£œå¯«æ™‚å¼·åˆ¶è¦†è“‹")

    args = parser.parse_args()
    if args.end is None:
        args.end = datetime.datetime.now().isoformat()
    elif isinstance(args.end, str) and args.end.lower() == "now":
        args.end = datetime.datetime.now().isoformat()

    if args.correction and (not args.start or not args.end):
        parser.error("--correction æ¨¡å¼å¿…é ˆæŒ‡å®š --start èˆ‡ --end")

    if args.force and not args.correction:
        parser.error("--force å¿…é ˆå’Œ --correction ä¸€èµ·ä½¿ç”¨")

    return args

def main():
    args = parse_args()
    global cp_data
    cp_data = load_checkpoint()

    # åªæœ‰åœ¨åŸ·è¡Œé·ç§»æœƒåˆå§‹åŒ–æ—¥èªŒåŠDBé€£ç·š
    if args.full_sync or args.incremental or args.correction or args.resume:
        init_logger()
        init_db_conn()

    if args.full_sync:
        command_full_sync(end_time=args.end)
    elif args.incremental:
        command_incremental(end_time=args.end)
    elif args.correction:
        command_correction(start_time=args.start, end_time=args.end, force=args.force)
    elif args.resume:
        command_resume()
    elif args.show_status:
        command_show_status()
    elif args.gen_report:
        command_gen_report()
    elif args.reset:
        command_reset()
    else:
        print("è«‹æŒ‡å®šæœ‰æ•ˆåƒæ•¸ï¼Œä½¿ç”¨ --help æŸ¥çœ‹èªªæ˜")

    # è¼¸å‡ºæ—¥èªŒè·¯å¾‘æ–¹ä¾¿æŸ¥çœ‹
    if LOG_FILE:
        print("\nğŸ” æœ¬æ¬¡é·ç§»æ—¥èªŒæ–‡ä»¶:")
        print(os.path.abspath(LOG_FILE))
        print(f"ä½¿ç”¨å‘½ä»¤æŸ¥çœ‹æ—¥èªŒï¼štail -f {os.path.abspath(LOG_FILE)}\n")

if __name__ == "__main__":
    main()
