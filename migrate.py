#!/usr/bin/env python3
# -*- coding: utf-8 -*-


"""
PostgreSQL â†’ MongoDB æ´»å‹•è³‡æ–™åˆå§‹åŒ–é·ç§»å·¥å…·ï¼ŒåŠŸèƒ½èªªæ˜ï¼š
- ä¸»è¡¨è¨ˆæ•¸ï¼Œå­è¡¨ç•°å¸¸ä¸å½±éŸ¿é·ç§»é€²åº¦
- æ”¯æ´æ–·é»çºŒå‚³ï¼ˆä»¥æ™‚é–“+event_no é›™éµï¼‰
- å¤§æ‰¹æ¬¡/ç²—ç²’åº¦æ‰¹æ¬¡åŠä¸€è‡´æ€§é©—è­‰
- å»¶é²æ—¥èªŒåˆå§‹åŒ–ï¼Œé¿å…æ²’æœ‰åƒæ•¸æ™‚ç”¢ç”Ÿç„¡æ•ˆæ—¥èªŒ
- å¤šç·šç¨‹æ‰¹é‡è™•ç†ï¼ŒåŠ é€ŸåŒæ­¥æ•ˆç‡
- å‘½ä»¤åˆ—å¤šæ¨¡å¼ï¼ˆå…¨é‡ã€å¢é‡ã€è£œå¯«ã€æ–·é»æ¢å¾©â€¦ï¼‰
"""


import os
import sys
import json
import logging
import datetime
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed


import psycopg2
from psycopg2.extras import DictCursor
from pymongo import MongoClient, UpdateOne
from dotenv import load_dotenv


# ===== è®€å–ç’°å¢ƒè®Šæ•¸èˆ‡è¼‰å…¥å°æ‡‰ .env æª”æ¡ˆ =====
ENV = os.getenv("ENV", "dev")
load_dotenv(f".env.{ENV}", override=True)


# PostgreSQL è³‡æ–™åº«è¨­å®š
POSTGRESQL_CONFIG = {
    "host": os.getenv("PG_HOST"),
    "port": int(os.getenv("PG_PORT", 5432)),
    "user": os.getenv("PG_USER"),
    "password": os.getenv("PG_PASSWORD"),
    "dbname": os.getenv("PG_DBNAME"),
}


# MongoDB é€£ç·šå­—ä¸²
MONGO_URI = os.getenv("MONGO_URI")


# æ‰¹æ¬¡å¤§å°èˆ‡æœ€å¤§ç·šç¨‹æ•¸
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "500"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "8"))
SCHEMA = "gift2022"


CHECKPOINT_FILE = "migration_checkpoint.json"  # æ–·é»æª”æ¡ˆ
LOG_DIR = "logs"                              # æ—¥èªŒç›®éŒ„


LOG_FILE = None   # æ—¥èªŒæª”å
logger = None     # logger å¯¦é«”


# ===== å»¶é²åˆå§‹åŒ–æ—¥èªŒç´€éŒ„å™¨ =====
def init_logger():
    """
    å•Ÿå‹•æ—¥èªŒç³»çµ±ï¼Œåœ¨ LOG_DIR ç›®éŒ„å»ºç«‹ä»¥ç•¶å‰æ™‚é–“å‘½åä¹‹æ—¥èªŒæ–‡ä»¶
    """
    global LOG_FILE, logger
    os.makedirs(LOG_DIR, exist_ok=True)
    LOG_FILE = os.path.join(LOG_DIR, f"migration_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    logging.basicConfig(
        filename=LOG_FILE,
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)s: %(message)s"
    )
    logger = logging.getLogger()


# ===== è³‡æ–™åº«å…¨å±€é€£ç·šç‰©ä»¶ =====
pg_conn = None          # PostgreSQL é€£ç·š
pg_cursor = None        # PostgreSQL æ¸¸æ¨™
mongo_client = None     # MongoDB é€£ç·š
col_events = None       # events é›†åˆ
col_attendees = None    # event_attendees é›†åˆ


def init_db_conn():
    """
    åˆå§‹åŒ– PostgreSQL å’Œ MongoDB é€£ç·šï¼Œä¸”å»ºç«‹å¿…è¦ç´¢å¼•
    """
    global pg_conn, pg_cursor, mongo_client, col_events, col_attendees
    try:
        pg_conn = psycopg2.connect(**POSTGRESQL_CONFIG, cursor_factory=DictCursor)
        pg_cursor = pg_conn.cursor()
    except Exception as e:
        logger.error(f"ç„¡æ³•é€£æ¥ PostgreSQLï¼š{e}")
        sys.exit(1)

    try:
        mongo_client = MongoClient(MONGO_URI)
        mdb = mongo_client["sogo_gift"]
        col_events = mdb["events"]
        col_attendees = mdb["event_attendees"]
        # ä¾å¸¸ç”¨æŸ¥è©¢åŠ å»ºç«‹ç´¢å¼•
        col_events.create_index([
            ("hccEventType", 1), ("eventStatus", 1), ("onlyApp", 1),
            ("startDate", 1), ("endDate", 1)
        ])
        col_events.create_index("memberTypes")
        col_attendees.create_index([("eventNo", 1), ("appId", 1)], unique=True)
    except Exception as e:
        logger.error(f"ç„¡æ³•é€£æ¥ MongoDBï¼š{e}")
        sys.exit(1)


# ===== åˆ†é¤¨åç¨±æ˜ å°„è¡¨ï¼ˆä¾›é¡¯ç¤ºåŠæ¯”å°æ’åºä½¿ç”¨ï¼‰ =====
BRANCH_ORDER = {
    "BRANCH_TAIWAN": "å…¨å°", "BRANCH_TAIPEI": "å°åŒ—åº—", "BRANCH_JS": "å¿ å­é¤¨",
    "BRANCH_FS": "å¾©èˆˆé¤¨", "BRANCH_DH": "æ•¦åŒ–é¤¨", "BRANCH_TM": "å¤©æ¯åº—",
    "BRANCH_JL": "ä¸­å£¢åº—", "BRANCH_BC": "æ–°ç«¹åº—", "BRANCH_GS": "é«˜é›„åº—",
    "BRANCH_GC": "Garden City", "BRANCH_GCA": "Garden City-Aå€",
    "BRANCH_GCB": "Garden City-Bå€", "BRANCH_GCC": "Garden City-Cå€", "BRANCH_GCD": "Garden City-Då€"
}


# ===== æ“´å……ä¸»æ´»å‹•è³‡æ–™ï¼Œæ›å…¥æœƒå“¡é¡å‹èˆ‡é©ç”¨åˆ†é¤¨è³‡è¨Š =====
def enrich_event(event):
    """
    æ“´å……æ´»å‹•è³‡æ–™å­—å…¸ï¼ŒåŸ·è¡Œä»¥ä¸‹æ“ä½œï¼š
    1. æŸ¥è©¢æ­¤æ´»å‹•å°æ‡‰çš„æœƒå“¡é¡å‹ï¼Œä¸¦åŠ å…¥ event["memberTypes"] åˆ—è¡¨
    2. æŸ¥è©¢æ­¤æ´»å‹•å°æ‡‰å„ªæƒ åˆ¸æ‰€é©ç”¨çš„åˆ†é¤¨æ¸…å–®ï¼Œ
       ä¸¦æ’åºå¾Œåˆ†åˆ¥å­˜å…¥ event["usingBranchIds"] (åˆ†é¤¨ID) èˆ‡ event["usingBranchNames"] (åˆ†é¤¨åç¨±)
    3. å°‡å­—ä¸²å‹å¸ƒæ—æ¬„ä½ï¼ˆonlyAppã€allMemberï¼‰è½‰æ›ç‚ºå¸ƒæ—å€¼ (True/False) æ–¹ä¾¿å¾ŒçºŒä½¿ç”¨

    ç•°å¸¸è™•ç†ï¼š
    - è‹¥æŸ¥è©¢æœƒå“¡é¡å‹æˆ–å„ªæƒ åˆ¸åˆ†é¤¨å¤±æ•—ï¼Œå‰‡å°æ‡‰æ¬„ä½è¨­ç‚ºç©ºåˆ—è¡¨ä¸¦è¨˜éŒ„è­¦å‘Šæ—¥èªŒ
    """

    event_no = event["eventNo"]  # å–å¾—æ´»å‹•ç·¨è™Ÿï¼Œä½œç‚ºæŸ¥è©¢æ¢ä»¶

    # æŸ¥è©¢æœƒå“¡é¡å‹ï¼Œè‹¥æœ‰å¤šç­†å‰‡èšåˆæˆåˆ—è¡¨è³¦å€¼çµ¦ event["memberTypes"]
    try:
        sql = f"SELECT member_type FROM {SCHEMA}.gif_hcc_event_member_type WHERE event_no = %s"
        logger.info(f"[SQL] æŸ¥æœƒå“¡é¡å‹: {sql} åƒæ•¸: [{event_no}]")
        pg_cursor.execute(sql, (event_no,))
        rows = pg_cursor.fetchall()
        event["memberTypes"] = [r["member_type"] for r in rows] if rows else []
    except Exception as e:
        # æŸ¥è©¢å¤±æ•—æ™‚ï¼Œè¨˜éŒ„è­¦å‘Šä¸¦è³¦å€¼ç©ºåˆ—è¡¨é¿å…ç¨‹å¼ä¸­æ–·
        logger.warning(f"æœƒå“¡é¡å‹æŸ¥è©¢å¤±æ•—ï¼Œäº‹ä»¶ {event_no}ï¼ŒéŒ¯èª¤ï¼š{e}")
        event["memberTypes"] = []

    # æŸ¥è©¢å„ªæƒ åˆ¸é©ç”¨åˆ†é¤¨ï¼Œåˆ©ç”¨å­æŸ¥è©¢å–å¾—æ´»å‹•é—œè¯coupon_setting_noï¼Œæ’ˆå–å°æ‡‰åˆ†é¤¨branch
    try:
        sql = f"""
            SELECT DISTINCT branch
            FROM {SCHEMA}.gif_event_coupon_burui
            WHERE coupon_setting_no IN (
                SELECT coupon_setting_no FROM {SCHEMA}.gif_event_coupon_setting WHERE event_no = %s
            )
        """
        logger.info(f"[SQL] æŸ¥å„ªæƒ åˆ¸åˆ†é¤¨: åƒæ•¸ [{event_no}]")
        pg_cursor.execute(sql, (event_no,))
        branches = [r["branch"] for r in pg_cursor.fetchall()]
        if branches:
            # æ ¹æ“šé è¨­åˆ†é¤¨é †åº BRANCH_ORDER æ’åºåˆ†é¤¨IDæ¸…å–®
            sorted_ids = sorted(branches, key=lambda b: list(BRANCH_ORDER.keys()).index(b) if b in BRANCH_ORDER else 9999)
            event["usingBranchIds"] = sorted_ids
            # å–å¾—å°æ‡‰ä¸­æ–‡åˆ†é¤¨åç¨±ï¼Œæ‰¾ä¸åˆ°æ™‚ä»¥IDä»£æ›¿
            event["usingBranchNames"] = [BRANCH_ORDER.get(b, b) for b in sorted_ids]
        else:
            # ç„¡åŒ¹é…è³‡æ–™å‰‡è¨­ç©ºåˆ—è¡¨
            event["usingBranchIds"] = []
            event["usingBranchNames"] = []
    except Exception as e:
        # æŸ¥è©¢å¤±æ•—æ™‚ï¼Œè¨˜éŒ„è­¦å‘Šä¸¦å°‡åˆ†é¤¨æ¬„ä½è¨­ç‚ºç©ºå€¼
        logger.warning(f"å„ªæƒ åˆ¸åˆ†é¤¨æŸ¥è©¢å¤±æ•—ï¼Œäº‹ä»¶ {event_no}ï¼ŒéŒ¯èª¤ï¼š{e}")
        event["usingBranchIds"] = []
        event["usingBranchNames"] = []

    # å°‡å­—ä¸²"Y"/"N"è½‰æ›ç‚ºå¸ƒæ—å€¼ True/False
    event["onlyApp"] = event.get("onlyApp", "N") == 'Y'
    event["allMember"] = event.get("allMember", "N") == 'Y'

    return event


# ===== æ‰¹æ¬¡æŠ“å–event_noï¼Œæ”¯æ´æ–·é»ï¼ˆé›™éµï¼šæ—¥æœŸ+event_noï¼‰ =====
def fetch_events_batch(start_time, end_time, last_checkpoint_time=None, last_checkpoint_id=None, batch_size=100):
    """
    æ‰¹æ¬¡å–å¾—æŒ‡å®šæ™‚é–“ç¯„åœå…§éœ€é·ç§»çš„æ´»å‹•ç·¨è™Ÿ (event_no)ã€‚

    æ”¯æ´ä¾æ™‚é–“æˆ³è¨˜åŠ äº‹ä»¶IDé›™éµé€²è¡Œæ–·é»çºŒå‚³ï¼Œ
    å¯é¿å…é‡è¤‡è™•ç†æˆ–éºæ¼è³‡æ–™ã€‚
    
    åƒæ•¸ï¼š
    - start_time: èµ·å§‹æ™‚é–“ï¼Œå­—ä¸²æˆ– datetime ç‰©ä»¶
    - end_time: çµæŸæ™‚é–“ï¼Œå­—ä¸²æˆ– datetime ç‰©ä»¶
    - last_checkpoint_time: ä¸Šä¸€æ¬¡æ–·é»æ™‚é–“ (ISOå­—ä¸²æˆ– datetime)ï¼Œä½œç‚ºä¸‹ä¸€æ‰¹æ¬¡çš„èµ·é»æ™‚é–“
    - last_checkpoint_id: ä¸Šä¸€æ¬¡æ–·é»äº‹ä»¶ç·¨è™Ÿ (event_no)ï¼Œè¼”åŠ©æ–·é»å®šä½
    - batch_size: å–®æ¬¡æŠ“å–æœ€å¤§æ´»å‹•æ•¸é‡é™åˆ¶

    å›å‚³ï¼š
    - list of event_noï¼Œç‚ºç¬¦åˆæ¢ä»¶çš„æ´»å‹•äº‹ä»¶ç·¨è™Ÿæ¸…å–®
    """

    # è‹¥è¼¸å…¥ç‚ºå­—ä¸²ï¼Œè½‰æ›æˆ datetime ç‰©ä»¶æ–¹ä¾¿æ¯”è¼ƒåŠ SQL å‚³åƒ
    if isinstance(start_time, str):
        start_time = datetime.datetime.fromisoformat(start_time)
    if isinstance(end_time, str):
        end_time = datetime.datetime.fromisoformat(end_time)

    # åˆ¤æ–·æ˜¯å¦ç‚ºæ–·é»çºŒå‚³æ¨¡å¼ï¼Œéœ€æŒ‡å®šä¸Šä¸€ç­†æ™‚é–“èˆ‡event_noç‚ºé›™éµ
    if last_checkpoint_time is not None and last_checkpoint_id is not None:
        sql = f"""
            SELECT event_no
            FROM {SCHEMA}.gif_event
            WHERE 
              -- å¾æ–·é»æ™‚é–“ä¸‹ä¸€ç­†èµ·ï¼Œä¾æ™‚é–“å…ˆå¾Œæ’åºè£œæŠ“
              (exchange_start_date > %s 
               OR (exchange_start_date = %s AND event_no > %s))
              -- ä¸”åœ¨æœ¬æ¬¡çª—å£æŒ‡å®šæ™‚é–“å€é–“å…§
              AND exchange_start_date >= %s AND exchange_start_date < %s
            ORDER BY exchange_start_date, event_no
            LIMIT %s
        """
        # SQLåƒæ•¸ä¾åºç‚ºï¼šæ–·é»æ™‚é–“ã€æ–·é»æ™‚é–“ã€æ–·é»event_noã€çª—å£èµ·å§‹ã€çª—å£çµæŸã€æŠ“å–æ•¸é‡ä¸Šé™
        params = [last_checkpoint_time, last_checkpoint_time, last_checkpoint_id, start_time, end_time, batch_size]
    else:
        # éæ–·é»æ¨¡å¼ï¼Œç›´æ¥åœ¨çª—å£æ™‚é–“ç¯„åœå…§ä¾æ™‚é–“åŠevent_noæ’åºæŠ“å–
        sql = f"""
            SELECT event_no
            FROM {SCHEMA}.gif_event
            WHERE exchange_start_date >= %s AND exchange_start_date < %s
            ORDER BY exchange_start_date, event_no
            LIMIT %s
        """
        params = [start_time, end_time, batch_size]

    # ç´€éŒ„ SQL æŸ¥è©¢èªå¥èˆ‡åƒæ•¸æ–¹ä¾¿è¿½è¹¤èˆ‡é™¤éŒ¯
    logger.info(f"[SQL] fetch_events_batch:\n{sql}\nåƒæ•¸: {params}")

    # åŸ·è¡Œ SQL æŸ¥è©¢
    pg_cursor.execute(sql, params)

    # å–å¾—æŸ¥è©¢çµæœ
    rows = pg_cursor.fetchall()

    # å›å‚³æ´»å‹•ç·¨è™Ÿåˆ—è¡¨ï¼Œä¾›å¾ŒçºŒé·ç§»ä½¿ç”¨
    return [row["event_no"] for row in rows]


# ===== å–å¾—æŒ‡å®šæ´»å‹•å®Œæ•´å…§å®¹ =====
def fetch_event(event_no):
    """
    å¾ PostgreSQL è³‡æ–™åº«ä¸­è¯æŸ¥ä¸»æ´»å‹•è¡¨èˆ‡é™„è¡¨ï¼Œ
    æ“·å–æŒ‡å®š event_no çš„å®Œæ•´æ´»å‹•è³‡æ–™ã€‚

    æŸ¥è©¢æ¬„ä½åŒ…å«ï¼š
    - ä¸»æ´»å‹•è¡¨ (gif_hcc_event) çš„ç´°ç¯€æ¬„ä½ï¼Œå¦‚äº‹ä»¶å‚™è¨»ã€åˆ†é¤¨ã€å„ªæƒ åˆ¸JSONã€æ³¨æ„äº‹é …é€£çµã€å¸ƒæ—æ¬„ä½ç­‰
    - é™„è¡¨ (gif_event) çš„å…±ç”¨æ¬„ä½ï¼Œå¦‚æ´»å‹•åç¨±ã€ç‹€æ…‹ã€é–‹å§‹çµæŸæ—¥æœŸåŠç¶²å€
    çµæœå°‡ä»¥å­—å…¸å½¢å¼è¿”å›ï¼Œæ¬„ä½åç¨±å·²æ ¼å¼åŒ–å°æ‡‰ã€‚

    è‹¥æ‰¾ä¸åˆ°è³‡æ–™å‰‡å›å‚³ Noneã€‚
    """

    # è¤‡åˆ SQL æŸ¥è©¢åŒæ™‚å–å¾—å…©è¡¨è³‡æ–™ï¼Œä¾ event_no åšé€£æ¥æ¢ä»¶
    sql = f"""
        SELECT he.event_no, he.event_memo, he.branch AS eventBranchId, he.prize_coupon_json,
               he.gift_attention_url, he.only_app, he.all_member, he.hcc_event_type,
               e.name, e.event_status, e.exchange_start_date, e.exchange_end_date, e.web_url
        FROM {SCHEMA}.gif_hcc_event he
        JOIN {SCHEMA}.gif_event e ON he.event_no = e.event_no
        WHERE he.event_no = %s
    """

    # è¨˜éŒ„ SQL åŸ·è¡Œè³‡è¨Šï¼Œæ–¹ä¾¿é™¤éŒ¯
    logger.info(f"[SQL] fetch_event {event_no}")

    # åŸ·è¡Œ SQLï¼Œå¸¶å…¥æ´»å‹•ç·¨è™Ÿåƒæ•¸
    pg_cursor.execute(sql, (event_no,))

    # å–å¾—å–®ç­†æŸ¥è©¢çµæœ
    row = pg_cursor.fetchone()

    # è‹¥ç„¡è³‡æ–™å‰‡å›å‚³ None
    if not row:
        return None

    # å°æ‡‰æŸ¥è©¢çµæœæ¬„ä½ï¼Œçµ„æˆå­—å…¸æ–¹ä¾¿å¾ŒçºŒè™•ç†
    keys = [
        "eventNo", "eventMemo", "eventBranchId", "prizeCouponJson", "giftAttentionUrl",
        "onlyApp", "allMember", "hccEventType", "name", "eventStatus",
        "startDate", "endDate", "giftInforUrl"
    ]

    # å°‡éµå€¼é…å°å›å‚³å­—å…¸
    return dict(zip(keys, row))


# ===== åŸ·è¡ŒæŸæ´»å‹•æ‰€æœ‰åƒèˆ‡è€…è³‡æ–™é·ç§»ï¼ˆå­è¡¨ï¼Œç•°å¸¸ä¸ä¸­æ–·ï¼‰ =====
def migrate_attendees(event_no):
    """
    é·ç§»æŒ‡å®šæ´»å‹•çš„æ‰€æœ‰åƒèˆ‡è€…è³‡æ–™è‡³ MongoDBã€‚

    ä¸»è¦æµç¨‹ï¼š
    1. å¾ PostgreSQL ä¸­æŸ¥è©¢è©²æ´»å‹•(event_no)æ‰€æœ‰åƒèˆ‡è€…çš„ app_id åˆ—è¡¨
    2. çµ„åˆ MongoDB æ‰¹æ¬¡ upsert è«‹æ±‚ (UpdateOne)ï¼Œ
       ä½¿ç”¨ eventNo èˆ‡ appId ä½œå”¯ä¸€éµç¢ºä¿è³‡æ–™ä¸é‡è¤‡
    3. ä½¿ç”¨ MongoDB çš„ bulk_write éåŒæ­¥åŸ·è¡Œæ‰¹æ¬¡æ“ä½œæå‡æ•ˆèƒ½
    4. å›å‚³æœ¬æ¬¡æˆåŠŸ upsertï¼ˆæ–°å¢ï¼‰ç­†æ•¸
    5. å¦‚æœ‰ç•°å¸¸ï¼Œç´€éŒ„è­¦å‘Šæ—¥èªŒä¸¦å›å‚³ 0ï¼Œé¿å…é˜»æ–·æ•´é«”é·ç§»æµç¨‹

    åƒæ•¸ï¼š
    - event_no: æ´»å‹•ç·¨è™Ÿ

    å›å‚³ï¼š
    - int: æˆåŠŸ upsert åƒèˆ‡è€…è³‡æ–™ç­†æ•¸
    """
    try:
        # SQL æŸ¥è©¢è©²æ´»å‹•å…¨éƒ¨åƒèˆ‡è€… app_id
        sql = f"SELECT app_id FROM {SCHEMA}.gif_hcc_event_attendee WHERE event_no = %s"
        logger.info(f"[SQL] migrate_attendees {event_no}")
        pg_cursor.execute(sql, (event_no,))
        rows = pg_cursor.fetchall()

        # è‹¥ç„¡åƒèˆ‡è€…è³‡æ–™ï¼Œç›´æ¥å›å‚³ 0
        if not rows:
            return 0

        requests = []
        # å°‡æŸ¥è©¢çµæœé€ç­†è½‰ç‚º MongoDB UpdateOne upsert è«‹æ±‚
        for r in rows:
            app_id = r.get("app_id")
            if app_id:
                requests.append(UpdateOne(
                    # æŸ¥è©¢æ¢ä»¶ï¼Œç¢ºä¿å”¯ä¸€åƒè€ƒeventNoèˆ‡appId
                    {"eventNo": event_no, "appId": app_id},
                    # è‹¥ä¸å­˜åœ¨å‰‡æ–°å¢ï¼Œä½¿ç”¨ $setOnInsert æ“ä½œç¬¦
                    {"$setOnInsert": {"eventNo": event_no, "appId": app_id}},
                    upsert=True
                ))

        # è‹¥ç„¡æœ‰æ•ˆè«‹æ±‚ï¼ˆç†è«–ä¸Šä¸æœƒï¼‰ï¼Œç›´æ¥å›å‚³ 0
        if not requests:
            return 0

        # åŸ·è¡Œ MongoDB æ‰¹æ¬¡å¯«å…¥ï¼Œordered=False å…è¨±ä¸¦è¡ŒåŸ·è¡Œä»¥æå‡æ•ˆç‡
        result = col_attendees.bulk_write(requests, ordered=False)

        # å›å‚³æœ¬æ¬¡ upsert æ–°å¢çš„ç­†æ•¸
        return result.upserted_count

    except Exception as e:
        # æ•ç²ç•°å¸¸ï¼Œè¨˜éŒ„è­¦å‘Šä»¥ç¤ºéè‡´å‘½éŒ¯èª¤ï¼Œä¸¦å›å‚³0
        # ç¢ºä¿å–®ç­†æ´»å‹•åƒèˆ‡è€…è³‡æ–™å¯«å…¥å¤±æ•—ä¸æœƒçµ‚æ­¢æ•´é«”é·ç§»æµç¨‹
        logger.warning(f"åƒèˆ‡è€…è³‡æ–™å¯«å…¥å¤±æ•—ï¼Œäº‹ä»¶ {event_no}ï¼ŒéŒ¯èª¤ï¼š{e}")
        return 0


# ===== é·ç§»å–®ä¸€æ´»å‹• (ä¸»è¡¨ã€é—œè¯å­è¡¨)ï¼Œçµ±ä¸€ç”¨ bulkWrite + upsert =====
def migrate_single_event(event_no):
    """
    é·ç§»å–®ä¸€æ´»å‹•è³‡æ–™èˆ‡å…¶åƒèˆ‡è€…è³‡æ–™ï¼Œå…¨éƒ¨ä½¿ç”¨ bulkWrite + upsert æ¨¡å¼ã€‚

    æµç¨‹ï¼š
    1. å–å¾—ä¸»æ´»å‹•è³‡æ–™ï¼Œè‹¥æ‰¾ä¸åˆ°å›å‚³ False
    2. æ“´å……æ´»å‹•è³‡æ–™ï¼ˆæœƒå“¡é¡å‹ã€åˆ†é¤¨ç­‰ï¼‰
    3. ç”¨ bulk_write å¯¦ç¾ update_one + upsertï¼Œè‹¥è³‡æ–™å­˜åœ¨æ›´æ–°ï¼Œå¦å‰‡æ–°å¢
    4. åŸ·è¡Œåƒèˆ‡è€…è³‡æ–™é·ç§»ï¼ˆåŒæ¨£æ‰¹æ¬¡ upsertï¼‰
    5. å›å‚³æˆåŠŸç‹€æ…‹
    """
    try:
        event = fetch_event(event_no)
        if not event:
            logger.warning(f"æ‰¾ä¸åˆ°æ´»å‹• {event_no}")
            return False

        event = enrich_event(event)

        # æº–å‚™æ›´æ–°è«‹æ±‚ï¼Œä½¿ç”¨ update_one + upsert ä¿è­‰å­˜åœ¨æ™‚æ›´æ–°ï¼Œä¸å­˜åœ¨æ™‚æ–°å¢
        request = UpdateOne(
            {"eventNo": event_no},
            {"$set": event},
            upsert=True
        )
        col_events.bulk_write([request], ordered=False)

        # åƒèˆ‡è€…è³‡æ–™çµ±ä¸€ç”¨ upsert æ‰¹é‡å¯«å…¥ï¼Œå› æœ‰å”¯ä¸€ç´¢å¼•ï¼Œä¸æœƒé‡è¤‡
        inserted = migrate_attendees(event_no)

        logger.info(f"æ´»å‹• {event_no} é·ç§»æˆåŠŸï¼Œåƒèˆ‡è€… {inserted} ç­†")
        return True
    except Exception as e:
        logger.error(f"æ´»å‹• {event_no} é·ç§»å¤±æ•—ï¼ŒéŒ¯èª¤ï¼š{e}")
        return False


# ===== çµ±è¨ˆæŒ‡å®šæ™‚é–“æ®µä¸»è¡¨æ´»å‹•è³‡æ–™æ•¸é‡ =====
def count_pg_events_in_window(start, end):
    """
    çµ±è¨ˆ PostgreSQL ä¸»è¡¨ä¸­ï¼ŒæŒ‡å®šæ™‚é–“å€é–“ (start åˆ° end) å…§çš„æ´»å‹•ç¸½ç­†æ•¸ã€‚

    åŠŸèƒ½èªªæ˜ï¼š
    - åˆ©ç”¨ INNER JOIN åŒæ­¥ä¸»æ´»å‹•è¡¨ (gif_hcc_event) èˆ‡é™„è¡¨ (gif_event)
      ä»¥ç¢ºä¿è¨ˆæ•¸æ´»å‹•å‡å­˜åœ¨æ–¼å…©è¡¨ä¸­
    - ä½¿ç”¨ exchange_start_date ä½œç‚ºæ™‚é–“ç¯„åœæ¢ä»¶éæ¿¾
    - è¿”å›è©²æ™‚é–“æ®µå…§æ´»å‹•çš„å®Œæ•´æ•¸é‡ï¼Œä½œç‚ºé·ç§»é€²åº¦æ ¡é©—ä¾æ“š

    åƒæ•¸ï¼š
    - start: èµ·å§‹æ™‚é–“ï¼Œdatetime æˆ– ISO8601 å­—ä¸²
    - end: çµæŸæ™‚é–“ï¼Œdatetime æˆ– ISO8601 å­—ä¸²

    å›å‚³ï¼š
    - int: ç¬¦åˆæ¢ä»¶çš„æ´»å‹•ç­†æ•¸
    """

    if isinstance(start, str):
        start = datetime.datetime.fromisoformat(start)
    if isinstance(end, str):
        end = datetime.datetime.fromisoformat(end)

    sql = f"""
        SELECT COUNT(*)
        FROM {SCHEMA}.gif_hcc_event he
        JOIN {SCHEMA}.gif_event e ON he.event_no = e.event_no
        WHERE e.exchange_start_date >= %s AND e.exchange_start_date < %s
    """

    pg_cursor.execute(sql, (start, end))

    return pg_cursor.fetchone()[0]


# ===== é·ç§»å–®å€‹çª—å£ï¼ŒåŒ…å«å¤šæ´»å‹• =====
def migrate_window(window: dict, window_name: str):
    """
    è™•ç†æŒ‡å®šã€Œæ™‚é–“çª—å£ã€å…§çš„æ´»å‹•è³‡æ–™é·ç§»ã€‚

    åŠŸèƒ½èªªæ˜ï¼š
    - æ ¹æ“šæ–·é»æ™‚é–“(last_checkpoint_time)å’Œäº‹ä»¶ID(last_checkpoint_id)æ‰¹æ¬¡æŠ“å–å¾…é·ç§»æ´»å‹•æ¸…å–®
    - å–®ç­†é·ç§»ä¸¦æŒçºŒæ›´æ–°æ–·é»è³‡æ–™ä»¥æ”¯æŒçºŒå‚³
    - ç›£æ§é€£çºŒå¤±æ•—æ¬¡æ•¸ï¼Œé¿å…æ­»å¾ªç’°
    - å®Œæˆå¾Œæ›´æ–°çª—å£ç‹€æ…‹ä¸¦è¨˜éŒ„åŸ·è¡Œæ™‚é–“èˆ‡çµ±è¨ˆ
    """
    logger.info(f"è™•ç†çª—å£ {window_name}ï¼š{window['start']} ~ {window['end']} ç‹€æ…‹ï¼š{window['status']}")

    if window.get("status") == "completed":
        logger.info(f"{window_name} å·²å®Œæˆï¼Œè·³é")
        return 0

    last_time = window.get("last_checkpoint_time")
    last_id = window.get("last_checkpoint_id")

    window["status"] = "in_progress"
    window["owner"] = os.uname().nodename
    window["start_exec_time"] = datetime.datetime.now().isoformat()
    save_checkpoint(cp_data)

    migrated = 0
    consecutive_fail_count = 0

    while True:
        batch_event_nos = fetch_events_batch(window["start"], window["end"], last_time, last_id, BATCH_SIZE)

        if not batch_event_nos:
            pg_total = count_pg_events_in_window(window["start"], window["end"])
            logger.info(f"PGä¸»è¡¨ç¸½æ•¸ï¼š{pg_total}ï¼Œå·²æˆåŠŸé·ç§»ï¼š{migrated}")

            if migrated >= pg_total:
                window["status"] = "completed"
                logger.info(f"{window_name} çª—å£å·²å®Œæˆ")
            else:
                window["status"] = "pending"
                logger.warning(f"{window_name} çª—å£å°šæœ‰ {pg_total - migrated} ç­†æœªé·ç§»")
            break

        logger.info(f"{window_name} æ‰¹æ¬¡å¤§å°ï¼š{len(batch_event_nos)}")

        for eno in batch_event_nos:
            success = migrate_single_event(eno)

            if success:
                migrated += 1
                last_id = eno

                pg_cursor.execute(f"SELECT exchange_start_date FROM {SCHEMA}.gif_event WHERE event_no = %s", (eno,))
                row = pg_cursor.fetchone()
                if row and row["exchange_start_date"]:
                    last_time = row["exchange_start_date"].isoformat()
                else:
                    logger.warning(f"å–event_noï¼š{eno} æ™‚é–“å¤±æ•—ï¼Œæ–·é»æ™‚é–“ä¸æ›´æ–°")

                window["last_checkpoint_time"] = last_time
                window["last_checkpoint_id"] = last_id
                window["processed_count"] = migrated
                window["last_update_time"] = datetime.datetime.now().isoformat()
                save_checkpoint(cp_data)

                consecutive_fail_count = 0
                logger.info(f"æ´»å‹• {eno} é·ç§»æˆåŠŸï¼Œæ–·é»æ™‚é–“æ›´æ–°: {last_time}")
            else:
                consecutive_fail_count += 1
                logger.warning(f"æ´»å‹• {eno} é·ç§»å¤±æ•—ï¼Œé€£çºŒéŒ¯èª¤è¨ˆæ•¸ {consecutive_fail_count}")

                if consecutive_fail_count >= 5:
                    logger.error(f"{window_name} é€£çºŒ 5 ç­†å¤±æ•—ï¼Œçµ‚æ­¢é·ç§»é¿å…æ­»å¾ªç’°")
                    return migrated

    window["finish_exec_time"] = datetime.datetime.now().isoformat()
    save_checkpoint(cp_data)

    pg_count = count_pg_events_in_window(window["start"], window["end"])
    fail_count = max(pg_count - migrated, 0)

    logger.info(f"\n-- {window_name} çª—å£é·ç§»ç¸½çµ --")
    logger.info(f"çª—æˆ¶æ™‚é–“ç¯„åœ: {window['start']} ~ {window['end']}")
    logger.info(f"PGä¸»è¡¨ç­†æ•¸: {pg_count}")
    logger.info(f"æˆåŠŸé·ç§»ç­†æ•¸: {migrated}")
    logger.info(f"å¤±æ•—ç­†æ•¸: {fail_count}")
    logger.info(f"çª—å£ç‹€æ…‹: {window['status']}\n")

    return migrated


# ===== æ–·é»è³‡æ–™çš„è¼‰å…¥/å„²å­˜ =====
def load_checkpoint():
    """
    å¾æª”æ¡ˆè¼‰å…¥ä¸Šæ¬¡åŒæ­¥æ–·é», ä¸å­˜åœ¨å‰‡åˆå§‹åŒ–ç©ºçµæ§‹
    """
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"base_windows": [], "correction_windows": []}


def save_checkpoint(data):
    """
    å°‡åŒæ­¥é€²åº¦èˆ‡ç‹€æ…‹å­˜æª”åˆ°checkpoint
    """
    with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ====== å‘½ä»¤è™•ç†ï¼ˆå…¨é‡ã€å¢é‡ã€è£œå¯«ã€æ¢å¾©ã€ç‹€æ…‹ç­‰æ¨¡å¼ï¼‰ =====
def command_full_sync(end_time=None):
    """
    å…¨é‡åŒæ­¥ï¼šå°‡è‡ª 1970-01-01ï¼ˆLinux ç´€å…ƒæ™‚é–“èµ·é»ï¼‰èµ·è‡³æŒ‡å®šçµæŸæ™‚é–“ç¯„åœå…§ï¼Œ
    æ‰€æœ‰æ´»å‹•è³‡æ–™ä¸€æ¬¡æ€§åŒæ­¥è‡³ç›®æ¨™è³‡æ–™åº«ï¼ˆMongoDBï¼‰ã€‚

    é€éæ™‚é–“å€é–“åŠƒåˆ†ã€ŒåŸºç·šçª—å£ã€ï¼ˆbase_windowsï¼‰ï¼Œä»¥æ‰¹æ¬¡æ–¹å¼é€çª—åŒæ­¥ï¼Œ
    å¦‚çª—å£ä¸å­˜åœ¨å‰‡æ–°å¢ä¸¦å„²å­˜æ–·é»ï¼Œæ”¯æ´æ–·é»çºŒå‚³åŠŸèƒ½ã€‚
    """

    if end_time is None:
        end_time = datetime.datetime.now().isoformat()

    global cp_data

    cp_data.setdefault("base_windows", [])

    exists = any(
        w["start"] == "1970-01-01T00:00:00" and w["end"] == end_time
        for w in cp_data["base_windows"]
    )

    if not exists:
        cp_data["base_windows"].append({
            "start": "1970-01-01T00:00:00",
            "end": end_time,
            "status": "pending",
            "owner": None,
            "processed_count": 0,
            "last_checkpoint_time": None,
            "last_checkpoint_id": None,
            "last_update_time": None,
            "start_exec_time": None,
            "finish_exec_time": None
        })
        save_checkpoint(cp_data)
        logger.info(f"æ–°å¢åŸºç·šçª—å£: 1970-01-01 è‡³ {end_time}")

    total = 0
    for idx, w in enumerate(cp_data["base_windows"]):
        if w["status"] in ("pending", "in_progress"):
            total += migrate_window(w, f"base_windows[{idx}]")

    print(f"å…¨é‡åŒæ­¥å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")


def command_incremental(end_time=None):
    """
    åŸ·è¡Œå¢é‡åŒæ­¥ä»»å‹™ï¼š
    - åªè™•ç†è‡ªæœ€å¾Œä¸€å€‹ base_windows çª—å£çš„çµæŸæ™‚é–“èµ·è‡³æŒ‡å®šæœ€æ–°æ™‚é–“å€é–“çš„æ–°å¢æˆ–è®Šæ›´è³‡æ–™
    - æ–°å¢ä¸€å€‹ pending ç‹€æ…‹çš„ base_windows çª—å£ä¾†ç¯©é¸é€™æ®µæ™‚é–“å…§éœ€é·ç§»çš„è³‡æ–™
    - æ­¤è¨­è¨ˆé¿å…é‡è¤‡é·ç§»èˆŠè³‡æ–™ï¼Œä¸¦ä¿æŒç‹€æ…‹é€æ˜å’Œè¿½è¹¤æ¸…æ™°
    """
    if end_time is None:
        end_time = datetime.datetime.now().isoformat()

    global cp_data

    cp_data.setdefault("base_windows", [])

    if cp_data["base_windows"]:
        last_window = max(cp_data["base_windows"], key=lambda w: w["end"])
        start_time = last_window["end"]
    else:
        start_time = "1970-01-01T00:00:00"

    cp_data["base_windows"].append({
        "start": start_time,
        "end": end_time,
        "status": "pending",
        "owner": None,
        "processed_count": 0,
        "last_checkpoint_time": None,
        "last_checkpoint_id": None,
        "last_update_time": None,
        "start_exec_time": None,
        "finish_exec_time": None
    })

    logger.info(f"æ–°å¢å¢é‡åŒæ­¥çª—å£ï¼š{start_time} è‡³ {end_time}")

    total = 0
    for idx, w in enumerate(cp_data["base_windows"]):
        if w["status"] in ("pending", "in_progress"):
            total += migrate_window(w, f"base_windows[{idx}]")

    save_checkpoint(cp_data)

    print(f"å¢é‡åŒæ­¥å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")


def command_correction(start_time, end_time, force=True):
    """
    è£œå¯«åŒæ­¥ï¼ˆæŒ‡å®šæ™‚é–“æ®µï¼‰, ç¾åƒ…ä¿ç•™å¼·åˆ¶è£œå¯«ï¼ˆè¦†è“‹ï¼‰æ¨¡å¼ã€‚
    """
    global cp_data
    cp_data.setdefault("correction_windows", [])
    cp_data["correction_windows"].append({
        "start": start_time,
        "end": end_time,
        "status": "pending",
        "owner": None,
        "processed_count": 0,
        "last_checkpoint_time": None,
        "last_checkpoint_id": None,
        "last_update_time": None,
        "resync": True,   # æ°¸é å¼·åˆ¶è¦†è“‹
        "start_exec_time": None,
        "finish_exec_time": None
    })
    save_checkpoint(cp_data)
    logger.info(f"æ–°å¢è£œå¯«çª—å£ï¼š{start_time} è‡³ {end_time} å¼·åˆ¶è¦†è“‹=çœŸ")

    total = 0
    for idx, w in enumerate(cp_data["correction_windows"]):
        if w["status"] in ("pending", "in_progress"):
            total += migrate_window(w, f"correction_windows[{idx}]")
    print(f"è£œå¯«åŒæ­¥å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")


def command_resume():
    """
    é‡å°æ‰€æœ‰ç‹€æ…‹ pending/in_progress çš„çª—å£æ¢å¾©çºŒå‚³
    """
    global cp_data
    total = 0
    for wlist in ("base_windows", "correction_windows"):
        for idx, w in enumerate(cp_data.get(wlist, [])):
            if w["status"] in ("pending", "in_progress"):
                total += migrate_window(w, f"{wlist}[{idx}]")
    print(f"æ–·é»æ¢å¾©å®Œæˆï¼Œé·ç§»ç­†æ•¸: {total}")


def command_show_status():
    """
    é¡¯ç¤ºç›®å‰æ–·é»èˆ‡çª—å£åŒæ­¥ç‹€æ…‹
    """
    global cp_data
    print(json.dumps(cp_data, ensure_ascii=False, indent=2))


def command_reset():
    """
    å±éšªæ“ä½œï¼Œé‡ç½®æ‰€æœ‰æ–·é»é€²åº¦æª”
    """
    confirm = input("è­¦å‘Šï¼šé‡ç½®æ–·é»å°‡æ¸…é™¤æ‰€æœ‰é·ç§»é€²åº¦ä¸”ç„¡æ³•æ¢å¾©ï¼Œè«‹è¼¸å…¥ Y ç¢ºèªï¼š")
    if confirm.strip().upper() == "Y":
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
        global cp_data
        cp_data = load_checkpoint()
        logger.info("æ–·é»æª”æ¡ˆå·²é‡ç½®")
        print("é‡ç½®å®Œæˆ")
    else:
        print("æ“ä½œå·²å–æ¶ˆ")


def command_gen_report():
    """
    è¼¸å‡ºå„æ™‚é–“çª—å£åŒæ­¥é€²åº¦æ‘˜è¦
    """
    global cp_data
    print("====== é·ç§»ç‹€æ…‹å ±å‘Š ======")
    for window_type in ("base_windows", "correction_windows"):
        print(f"\n[{window_type}] æ™‚é–“çª—å£:")
        for w in cp_data.get(window_type, []):
            print(f"  æ™‚æ®µ: {w['start']} ~ {w['end']}")
            print(f"  ç‹€æ…‹: {w['status']}, å·²é·ç§»ç­†æ•¸: {w.get('processed_count', 0)}")
            print(f"  æœ€å¾Œæ–·é»æ™‚é–“: {w.get('last_checkpoint_time')}")
            print(f"  æœ€å¾Œæ–·é»äº‹ä»¶ID: {w.get('last_checkpoint_id')}")
            print(f"  åŸ·è¡Œè€…: {w.get('owner')}")
            print(f"  æœ€å¾Œæ›´æ–°æ™‚é–“: {w.get('last_update_time')}")
    print("============================")


# ====== å‘½ä»¤åˆ—åƒæ•¸è™•ç† =====
def parse_args():
    """
    è§£æå‘½ä»¤åˆ—åƒæ•¸ï¼Œæ”¯æ´å„åŸ·è¡Œæ¨¡å¼
    """
    parser = argparse.ArgumentParser(description="PostgreSQL â†’ MongoDB æ´»å‹•è³‡æ–™é·ç§»å·¥å…·")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--full-sync", action="store_true", help="å…¨é‡åŒæ­¥")
    group.add_argument("--incremental", action="store_true", help="å¢é‡åŒæ­¥")
    group.add_argument("--correction", action="store_true", help="å¼·åˆ¶è£œå¯«çª—å£")
    group.add_argument("--resume", action="store_true", help="æ–·é»æ¢å¾©")
    group.add_argument("--show-status", action="store_true", help="æŸ¥çœ‹æ–·é»ç‹€æ…‹")
    group.add_argument("--gen-report", action="store_true", help="ç”Ÿæˆé·ç§»å ±å‘Š")
    group.add_argument("--reset", action="store_true", help="é‡ç½®æ‰€æœ‰æ–·é»")

    parser.add_argument("--start", type=str, default=None, help="è£œå¯«çª—å£èµ·å§‹æ™‚é–“ï¼ˆISO8601æ ¼å¼ï¼‰")
    parser.add_argument("--end", type=str, default=None, help="çµæŸæ™‚é–“ï¼ˆISO8601æ ¼å¼æˆ– nowï¼Œé è¨­ç•¶å‰æ™‚é–“ï¼‰")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶è£œå¯«æ™‚å¿…é ˆåŠ ")

    args = parser.parse_args()
    if args.end is None:
        args.end = datetime.datetime.now().isoformat()
    elif isinstance(args.end, str) and args.end.lower() == "now":
        args.end = datetime.datetime.now().isoformat()

    if args.correction and (not args.start or not args.end):
        parser.error("--correction æ¨¡å¼å¿…é ˆæŒ‡å®š --start èˆ‡ --end")

    if args.force and not args.correction:
        parser.error("--force å¿…é ˆå’Œ --correction ä¸€èµ·ä½¿ç”¨")

    return args


# ===== ä¸»é€²å…¥é» =====
def main():
    args = parse_args()
    global cp_data
    cp_data = load_checkpoint()

    if args.full_sync or args.incremental or args.correction or args.resume:
        init_logger()
        init_db_conn()

    if args.full_sync:
        command_full_sync(end_time=args.end)
    elif args.incremental:
        command_incremental(end_time=args.end)
    elif args.correction:
        # åªä¿ç•™å¼·åˆ¶è£œå¯«æ¨¡å¼ï¼Œforce åƒæ•¸å¿…é ˆæä¾›ä¸”ç‚º Trueï¼Œåƒæ•¸åœ¨å‘½ä»¤åˆ—å¿…é ˆæä¾›
        command_correction(start_time=args.start, end_time=args.end, force=args.force)
    elif args.resume:
        command_resume()
    elif args.show_status:
        command_show_status()
    elif args.gen_report:
        command_gen_report()
    elif args.reset:
        command_reset()
    else:
        print("è«‹æŒ‡å®šæœ‰æ•ˆåƒæ•¸ï¼Œä½¿ç”¨ --help æŸ¥çœ‹èªªæ˜")

    if LOG_FILE:
        print("\nğŸ” æœ¬æ¬¡é·ç§»æ—¥èªŒæ–‡ä»¶:")
        print(os.path.abspath(LOG_FILE))
        print(f"ä½¿ç”¨å‘½ä»¤æŸ¥çœ‹æ—¥èªŒï¼štail -f {os.path.abspath(LOG_FILE)}\n")


if __name__ == "__main__":
    main()
